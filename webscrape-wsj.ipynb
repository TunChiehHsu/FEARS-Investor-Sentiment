{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.keys import Keys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import re\n",
    "import os\n",
    "import time\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extract Url"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## Open WSJ homepage and log in : \n",
    "chrome_url = input(\"Enter the location of your chromedriver:\"+\"\\n\")\n",
    "driver = webdriver.Chrome(chrome_url)\n",
    "driver.get('http://www.wsj.com')\n",
    "login = driver.find_element_by_link_text(\"Sign In\").click()\n",
    "\n",
    "username = input(\"type the username for wsj: \")\n",
    "password = input(\"type the password for wsj: \")\n",
    "\n",
    "time.sleep(2)\n",
    "loginID = driver.find_element_by_id(\"username\").send_keys(username)\n",
    "loginPass = driver.find_element_by_id(\"password\").send_keys(password)\n",
    "loginReady = driver.find_element_by_class_name(\"basic-login-submit\")\n",
    "loginReady.submit()\n",
    "\n",
    "def u_url(year,month,day):\n",
    "    return \"http://www.wsj.com/public/page/archive-\" + str(year) + \"-\" + str(month) + \"-\" + str(day) + \".html\"\n",
    "\n",
    "def getPageUrl(elementLinks):\n",
    "    extractLinks = []\n",
    "    for element in elementLinks:\n",
    "        links = element.get_attribute('href')\n",
    "        extractLinks.append(links)\n",
    "    return(extractLinks)\n",
    "\n",
    "normal_year = {1:31,2:28,3:31,4:30,5:31,6:30,7:31,8:31,9:30,10:31,11:30,12:31}\n",
    "leap_year = {1:31,2:29,3:31,4:30,5:31,6:30,7:31,8:31,9:30,10:31,11:30,12:31}               \n",
    "\n",
    "year = int(input(\"The year you want to extract the link for wsj: \"))\n",
    "article_link = []\n",
    "for month in range(1,13):\n",
    "    if (year % 4 == 0 and year % 100 != 0) or (year % 400 == 0):\n",
    "        year_type = leap_year\n",
    "    else:\n",
    "        year_type = normal_year\n",
    "    for day in range(1,year_type[month]+1):\n",
    "        url = u_url(year,month,day)\n",
    "        driver.get(url)\n",
    "        time.sleep(1)\n",
    "        element = driver.find_elements_by_xpath('//ul[@class = \"newsItem\"]//a')\n",
    "        link = getPageUrl(element)\n",
    "        article_link.append(link)\n",
    "        if day % 10 == 0:\n",
    "            print(\"month:\"+ str(month) + \" \" \"day:\" + str(day))\n",
    "            \n",
    "article_link = [y for x in article_link for y in x]\n",
    "f_name = \"wsj_\" + str(year) + \"_link.txt\"\n",
    "f = open(f_name,'w')\n",
    "for i in article_link:\n",
    "    f.write(i)\n",
    "    f.write(\"\\n\")\n",
    "f.close()\n",
    "\n",
    "print(\"The number of link extracted: \" + str(len(article_link)))\n",
    "print(\"link extraction complete!\")\n",
    "\n",
    "o_c = input(\"Do you want to close Chromedriver?(type Y or N)\")\n",
    "if o_c.lower() == \"y\":\n",
    "    driver.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extract Article"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "##%%writefile wsj_scrape.py\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.keys import Keys\n",
    "\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "\n",
    "\n",
    "import re\n",
    "import time\n",
    "import numpy as np\n",
    "\n",
    "## Open WSJ homepage and log in : \n",
    "chrome_url = input(\"Enter the location of your chromedriver:\"+\"\\n\")\n",
    "driver = webdriver.Chrome(chrome_url)\n",
    "driver.get('http://www.wsj.com')\n",
    "login = driver.find_element_by_link_text(\"Sign In\").click()\n",
    "\n",
    "username = input(\"type the username for wsj: \")\n",
    "password = input(\"type the password for wsj: \")\n",
    "\n",
    "time.sleep(2)\n",
    "loginID = driver.find_element_by_id(\"username\").send_keys(username)\n",
    "loginPass = driver.find_element_by_id(\"password\").send_keys(password)\n",
    "loginReady = driver.find_element_by_class_name(\"basic-login-submit\")\n",
    "loginReady.submit()\n",
    "link_file = input(\"Enter the name of link file(without .txt): \")+ \".txt\"\n",
    "out_file  = input(\"Enter the name for output file(without .txt): \")+ \".txt\"\n",
    "junk_file = input(\"Enter the name for junk file(without .txt): \")+ \".txt\"\n",
    "cur_type  = input(\"Do you want to start from the beginning(y or n): \")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "if cur_type.lower() == \"y\":\n",
    "    cur_num = 1\n",
    "    t_type    = input(\"Do you want to extract articles for all links(type Y or N): \")\n",
    "    if t_type.lower() == \"y\":\n",
    "        number = float('inf')\n",
    "    else:\n",
    "        number = float(input(\"Type the number of articles you want to extract: \"))\n",
    "        \n",
    "else:\n",
    "    cur_num = int(input(\"Enter the number you want to start from: \"))\n",
    "    number = float(input(\"Type the number of articles you want to extract: \"))\n",
    "\n",
    "start_time = time.time()\n",
    "count = 0\n",
    "effective_count = 0\n",
    "with open(link_file, 'r') as infile, open(out_file,\"a\") as outfile,open(junk_file,\"a\") as junkfile:\n",
    "    for link in infile:\n",
    "        \n",
    "        if count < cur_num-1:\n",
    "            count+=1\n",
    "            continue\n",
    "            \n",
    "        if count - (cur_num-1) < number:\n",
    "            driver.get(link)\n",
    "            count+=1\n",
    "        \n",
    "            \n",
    "            ##extract tag\n",
    "            tt = []\n",
    "            try:\n",
    "                tag = driver.find_elements_by_class_name(\"article-breadCrumb\")\n",
    "                if tag == []:\n",
    "                    print(\"This article has no tag, may not be an article: \" + link)\n",
    "                    junkfile.write(link + \"\\n\")\n",
    "                    continue\n",
    "                for t in tag:\n",
    "                    outfile.write(\"tag_g: \")\n",
    "                    outfile.write(t.text + \" \")\n",
    "            except NoSuchElementException:\n",
    "                print(\"This article has no tag, may not be an article: \" + link)\n",
    "                junkfile.write(link + \"\\n\")\n",
    "                continue\n",
    "            \n",
    "            ##extract headline\n",
    "            try:\n",
    "                headline = driver.find_element_by_class_name(\"wsj-article-headline\").text\n",
    "                outfile.write(\"headline_h: \"+headline+\" \")\n",
    "                effective_count += 1\n",
    "            except NoSuchElementException:\n",
    "                print(\"This article has no headline, may not be an article: \" + link)\n",
    "                junkfile.write(link + \"\\n\")\n",
    "                continue\n",
    "            \n",
    "            \n",
    "            ##extract time\n",
    "            try:\n",
    "                timestamp = driver.find_element_by_class_name(\"timestamp\").text\n",
    "            except NoSuchElementException:\n",
    "                print(\"This article has no time stamp, may not be an article: \" + link)\n",
    "                junkfile.write(link + \"\\n\")\n",
    "                continue\n",
    "            # clean time stamp if it exists \n",
    "            timestamp = re.sub(r'Updated ', '', timestamp)\n",
    "            timestamp = re.sub(r' ET', '', timestamp)\n",
    "            timestamp = re.sub(r'p.m.', 'PM', timestamp)\n",
    "            timestamp = re.sub(r'a.m.', 'AM', timestamp)\n",
    "            outfile.write(\"time_t: \"+ timestamp +\"\\n\")\n",
    "            \n",
    "            ##extract article text\n",
    "            paragraphs = driver.find_elements_by_xpath('//*[@id=\"wsj-article-wrap\"]/p')\n",
    "            text = []\n",
    "            if paragraphs == []:\n",
    "                print(\"This article has no text, may not be an article: \" + link)\n",
    "                junkfile.write(link + \"\\n\")\n",
    "                continue\n",
    "            outfile.write(link)\n",
    "            for tt in paragraphs:\n",
    "                if('@wsj.com' not in tt.text and 'contributed to this article' not in tt.text):\n",
    "                    text.append(tt.text)\n",
    "            text = \"\".join(text)\n",
    "            text = re.sub(r'\\n',\" \",text)\n",
    "            outfile.write(text.lower() + \"\\n\")\n",
    "            outfile.write(\"++++++++++++++++++++++++++\"+ \"\\n\")\n",
    "            \n",
    "            ##print \n",
    "            if number < 1000:\n",
    "                d = 10\n",
    "            else:\n",
    "                d = 100\n",
    "            if (count - cur_num) % d == 0:\n",
    "                print(\"# extract article: \" + str(count - cur_num + 1))\n",
    "            if effective_count % d == 0:\n",
    "                print(\"# extract effective article: \" + str(effective_count))\n",
    "            time.sleep(0.5)\n",
    "end_time = time.time()\n",
    "print(\"Time spent: \" + str(np.round((end_time - start_time),3)) + \"s\")\n",
    "print(\"Total number of article:\" + str(count - cur_num + 1))\n",
    "print(\"Total number of effective article:\" + str(effective_count))\n",
    "print(\"You should start from {} next time\".format(count+1))\n",
    "print(\"Article extraction complete!\")\n",
    "o_c = input(\"Do you want to close Chromedriver?(type Y or N)\")\n",
    "if o_c.lower() == \"y\":\n",
    "    driver.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
