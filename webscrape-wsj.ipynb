{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.keys import Keys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import re\n",
    "import os\n",
    "import time\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test: open and extract information from python website"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "driver = webdriver.Chrome(\"/Users/zhanghengqian/NY-times-scraping/chromedriver\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "driver.get(\"http://www.python.org\")\n",
    "assert \"Python\" in driver.title\n",
    "elem = driver.find_element_by_name(\"q\")\n",
    "elem.clear()\n",
    "elem.send_keys(\"pycon\")\n",
    "elem.send_keys(Keys.RETURN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "driver.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test: search and extract information from WSJ website"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## Open WSJ homepage and log in : \n",
    "chrome_url = input(\"Enter the location of your chromedriver:\"+\"\\n\")\n",
    "driver = webdriver.Chrome(chrome_url)\n",
    "driver.get('http://www.wsj.com')\n",
    "login = driver.find_element_by_link_text(\"Sign In\").click()\n",
    "\n",
    "username = input(\"type the username for wsj: \")\n",
    "password = input(\"type the password for wsj: \")\n",
    "\n",
    "time.sleep(2)\n",
    "loginID = driver.find_element_by_id(\"username\").send_keys(username)\n",
    "loginPass = driver.find_element_by_id(\"password\").send_keys(password)\n",
    "loginReady = driver.find_element_by_class_name(\"basic-login-submit\")\n",
    "loginReady.submit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cookie agreement already acknowledged\n"
     ]
    }
   ],
   "source": [
    "## Close cookie policy if needed\n",
    "try:\n",
    "    driver.find_element_by_class_name(\"close\").click()\n",
    "except NoSuchElementException:\n",
    "    print('Cookie agreement already acknowledged')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<selenium.webdriver.remote.webelement.WebElement (session=\"aaa035cbe8708192e6e428743be3df40\", element=\"0.2121809510043231-1\")>"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## Basic search: articles containing country in the Journal since 2012 \n",
    "WebDriverWait(driver, 10).until(\n",
    "        EC.presence_of_element_located((By.ID, \"globalHatSearchInput\"))\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "search_box = driver.find_element_by_id(\"globalHatSearchInput\")\n",
    "search_box.clear()\n",
    "search_box.send_keys('Economy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<selenium.webdriver.support.wait.WebDriverWait (session=\"aaa035cbe8708192e6e428743be3df40\")>"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "WebDriverWait(driver, 5)\n",
    "search_req = driver.find_element_by_css_selector('.button-search').click()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cookie agreement already acknowledged\n"
     ]
    }
   ],
   "source": [
    "## Close cookie policy if needed\n",
    "try:\n",
    "    driver.find_element_by_class_name(\"close\").click()\n",
    "except NoSuchElementException:\n",
    "    print('Cookie agreement already acknowledged')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "toggleMenu = driver.find_element_by_link_text(\"ADVANCED SEARCH\")\n",
    "toggleMenu.click()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "menuOptions = driver.find_element_by_class_name('datePeriod')\n",
    "driver.find_element_by_name(\"sfrom\").send_keys(\"2010/08/25\")\n",
    "driver.find_element_by_name(\"sto\").send_keys(\"2017/12/25\")\n",
    "driver.find_element_by_id('metadata').send_keys(\"Economy\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## Restrict search to articles only (exclude videos, blogs, etc)\n",
    "driver.execute_script(\"window.scrollTo(0, 500)\")\n",
    "driver.find_element_by_link_text(\"WSJ Videos\").click()\n",
    "driver.find_element_by_link_text(\"WSJ Site Search\").click()\n",
    "driver.find_element_by_link_text(\"WSJ Blogs\").click()\n",
    "driver.execute_script(\"window.scrollTo(0, 0)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "searchArchive = driver.find_element_by_class_name('keywordSearchBar')\n",
    "searchArchive.find_element_by_class_name(\"searchButton\").click()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "pageCount = driver.find_elements_by_class_name(\"results-count\")[1].text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "resultCount = driver.find_elements_by_class_name(\"results-count\")[0].text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "resultCount = int(resultCount.rpartition(\"of \")[2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## Extract all article urls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#driver.maximize_window()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def getPageUrl(elementLinks):\n",
    "    extractLinks = []\n",
    "    for element in elementLinks:\n",
    "        links = element.get_attribute('href')\n",
    "        extractLinks.append(links)\n",
    "    return(extractLinks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "time.sleep(0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# ==============================================================================\n",
    "## Extract all article urls\n",
    "##browser.maximize_window()\n",
    "articleLinks = []\n",
    "for j in range(0, pageCount):\n",
    "    elementLinks = driver.find_elements_by_xpath('//h3[@class=\"headline\"]/a')\n",
    "    links = getPageUrl(elementLinks)\n",
    "    articleLinks.append(links)\n",
    "    print('done with page ' + str(j+1) + ' of ' + str(pageCount))\n",
    "    time.sleep(1)\n",
    "    if j < pageCount:\n",
    "        element = driver.find_element_by_class_name('next-page')\n",
    "        driver.execute_script(\"arguments[0].click();\", element)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "311"
      ]
     },
     "execution_count": 122,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(articleLinks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6216"
      ]
     },
     "execution_count": 126,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(articleLinks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "articleLinks = [y for x in articleLinks for y in x]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "driver.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extract Url"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "##%%writefile wsj_scrape_link.py\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.keys import Keys\n",
    "\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "\n",
    "\n",
    "import re\n",
    "import time\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "\n",
    "## Open WSJ homepage and log in : \n",
    "chrome_url = input(\"Enter the location of your chromedriver:\"+\"\\n\")\n",
    "driver = webdriver.Chrome(chrome_url)\n",
    "driver.get('http://www.wsj.com')\n",
    "login = driver.find_element_by_link_text(\"Sign In\").click()\n",
    "\n",
    "username = input(\"type the username for wsj: \")\n",
    "password = input(\"type the password for wsj: \")\n",
    "\n",
    "time.sleep(2)\n",
    "loginID = driver.find_element_by_id(\"username\").send_keys(username)\n",
    "loginPass = driver.find_element_by_id(\"password\").send_keys(password)\n",
    "loginReady = driver.find_element_by_class_name(\"basic-login-submit\")\n",
    "loginReady.submit()\n",
    "\n",
    "def u_url(year,month,day):\n",
    "    return \"http://www.wsj.com/public/page/archive-\" + str(year) + \"-\" + str(month) + \"-\" + str(day) + \".html\"\n",
    "\n",
    "def getPageUrl(elementLinks):\n",
    "    extractLinks = []\n",
    "    for element in elementLinks:\n",
    "        links = element.get_attribute('href')\n",
    "        extractLinks.append(links)\n",
    "    return(extractLinks)\n",
    "\n",
    "normal_year = {1:31,2:28,3:31,4:30,5:31,6:30,7:31,8:31,9:30,10:31,11:30,12:31}\n",
    "leap_year = {1:31,2:29,3:31,4:30,5:31,6:30,7:31,8:31,9:30,10:31,11:30,12:31}               \n",
    "\n",
    "year = int(input(\"The year you want to extract the link for wsj: \"))\n",
    "article_link = []\n",
    "for month in range(1,13):\n",
    "    if (year % 4 == 0 and year % 100 != 0) or (year % 400 == 0):\n",
    "        year_type = leap_year\n",
    "    else:\n",
    "        year_type = normal_year\n",
    "    for day in range(1,year_type[month]+1):\n",
    "        url = u_url(year,month,day)\n",
    "        driver.get(url)\n",
    "        time.sleep(1)\n",
    "        element = driver.find_elements_by_xpath('//ul[@class = \"newsItem\"]//a')\n",
    "        link = getPageUrl(element)\n",
    "        article_link.append(link)\n",
    "        if day % 10 == 0:\n",
    "            print(\"month:\"+ str(month) + \" \" \"day:\" + str(day))\n",
    "            \n",
    "article_link = [y for x in article_link for y in x]\n",
    "f_name = \"wsj_\" + str(year) + \"_link.txt'\"\n",
    "f = open(f_name,'w')\n",
    "for i in article_link:\n",
    "    f.write(i)\n",
    "    f.write(\"\\n\")\n",
    "f.close()\n",
    "\n",
    "print(\"The number of link extracted: \" + str(len(article_link)))\n",
    "print(\"link extraction complete!\")\n",
    "\n",
    "o_c = input(\"Do you want to close Chromedriver?(type Y or N)\")\n",
    "if o_c.lower() == \"y\":\n",
    "    driver.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extract Article"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "##%%writefile wsj_scrape.py\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.keys import Keys\n",
    "\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "\n",
    "\n",
    "import re\n",
    "import time\n",
    "import numpy as np\n",
    "\n",
    "## Open WSJ homepage and log in : \n",
    "chrome_url = input(\"Enter the location of your chromedriver:\"+\"\\n\")\n",
    "driver = webdriver.Chrome(chrome_url)\n",
    "driver.get('http://www.wsj.com')\n",
    "login = driver.find_element_by_link_text(\"Sign In\").click()\n",
    "\n",
    "username = input(\"type the username for wsj: \")\n",
    "password = input(\"type the password for wsj: \")\n",
    "\n",
    "time.sleep(2)\n",
    "loginID = driver.find_element_by_id(\"username\").send_keys(username)\n",
    "loginPass = driver.find_element_by_id(\"password\").send_keys(password)\n",
    "loginReady = driver.find_element_by_class_name(\"basic-login-submit\")\n",
    "loginReady.submit()\n",
    "link_file = input(\"Enter the name of link file(without .txt): \")+ \".txt\"\n",
    "out_file  = input(\"Enter the name for output file(without .txt): \")+ \".txt\"\n",
    "junk_file = input(\"Enter the name for junk file(without .txt): \")+ \".txt\"\n",
    "t_type    = input(\"Do you want to extract articles for all links(type Y or N): \")\n",
    "if t_type.lower() == \"y\":\n",
    "    number = float('inf')\n",
    "else:\n",
    "    number = float(input(\"Type the number of articles you want to extract: \"))\n",
    "start_time = time.time()\n",
    "count = 0\n",
    "effective_count = 0\n",
    "with open(link_file, 'r') as infile, open(out_file,\"w\") as outfile,open(junk_file,\"w\") as junkfile:\n",
    "    for link in infile:\n",
    "        if count < number:\n",
    "            driver.get(link)\n",
    "            count += 1\n",
    "            \n",
    "            ##extract tag\n",
    "            tt = []\n",
    "            try:\n",
    "                tag = driver.find_elements_by_class_name(\"article-breadCrumb\")\n",
    "                if tag == []:\n",
    "                    print(\"This article has no tag, may not be an article: \" + link)\n",
    "                    junkfile.write(link + \"\\n\")\n",
    "                    continue\n",
    "                for t in tag:\n",
    "                    outfile.write(\"tag_g: \")\n",
    "                    outfile.write(t.text + \" \")\n",
    "            except NoSuchElementException:\n",
    "                print(\"This article has no tag, may not be an article: \" + link)\n",
    "                junkfile.write(link + \"\\n\")\n",
    "                continue\n",
    "            \n",
    "            ##extract headline\n",
    "            try:\n",
    "                headline = driver.find_element_by_class_name(\"wsj-article-headline\").text\n",
    "                outfile.write(\"headline_h: \"+headline+\" \")\n",
    "                effective_count += 1\n",
    "            except NoSuchElementException:\n",
    "                print(\"This article has no headline, may not be an article: \" + link)\n",
    "                junkfile.write(link + \"\\n\")\n",
    "                continue\n",
    "            \n",
    "            \n",
    "            ##extract time\n",
    "            try:\n",
    "                timestamp = driver.find_element_by_class_name(\"timestamp\").text\n",
    "            except NoSuchElementException:\n",
    "                print(\"This article has no time stamp, may not be an article: \" + link)\n",
    "                junkfile.write(link + \"\\n\")\n",
    "                continue\n",
    "            # clean time stamp if it exists \n",
    "            timestamp = re.sub(r'Updated ', '', timestamp)\n",
    "            timestamp = re.sub(r' ET', '', timestamp)\n",
    "            timestamp = re.sub(r'p.m.', 'PM', timestamp)\n",
    "            timestamp = re.sub(r'a.m.', 'AM', timestamp)\n",
    "            outfile.write(\"time_t: \"+ timestamp +\"\\n\")\n",
    "            \n",
    "            ##extract article text\n",
    "            paragraphs = driver.find_elements_by_xpath('//*[@id=\"wsj-article-wrap\"]/p')\n",
    "            text = []\n",
    "            if paragraphs == []:\n",
    "                print(\"This article has no text, may not be an article: \" + link)\n",
    "                junkfile.write(link + \"\\n\")\n",
    "                continue\n",
    "            outfile.write(link)\n",
    "            for tt in paragraphs:\n",
    "                if('@wsj.com' not in tt.text and 'contributed to this article' not in tt.text):\n",
    "                    text.append(tt.text)\n",
    "            text = \"\".join(text)\n",
    "            text = re.sub(r'\\n',\" \",text)\n",
    "            outfile.write(text.lower() + \"\\n\")\n",
    "            outfile.write(\"++++++++++++++++++++++++++\"+ \"\\n\")\n",
    "            \n",
    "            ##print \n",
    "            if number < 1000:\n",
    "                d = 10\n",
    "            else:\n",
    "                d = 100\n",
    "            if count % d == 0:\n",
    "                print(\"# extract article: \" + str(count))\n",
    "            if effective_count % d == 0:\n",
    "                print(\"# extract effective article: \" + str(effective_count))\n",
    "            time.sleep(0.5)\n",
    "end_time = time.time()\n",
    "print(\"Time spent: \" + str(np.round((end_time - start_time),3)) + \"s\")\n",
    "print(\"Total number of article:\" + str(count))\n",
    "print(\"Total number of effective article:\" + str(effective_count))\n",
    "print(\"Article extraction complete!\")\n",
    "o_c = input(\"Do you want to close Chromedriver?(type Y or N)\")\n",
    "if o_c.lower() == \"y\":\n",
    "    driver.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
