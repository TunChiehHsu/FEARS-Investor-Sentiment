{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "from multiprocessing import Pool\n",
    "import re\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem.snowball import SnowballStemmer\n",
    "import pickle\n",
    "import itertools\n",
    "import time\n",
    "import os\n",
    "from nltk import word_tokenize\n",
    "\n",
    "\n",
    "def text_to_wordlist(text, remove_stopwords=False, stem_words=False):\n",
    "    \n",
    "    # Clean the text, with the option to remove stopwords and to stem words.\n",
    "    \n",
    "    \n",
    "    # Convert words to lower case and split them\n",
    "    text = text.lower().split()\n",
    "\n",
    "    # Optionally, remove stop words\n",
    "    if remove_stopwords:\n",
    "        stops = set(stopwords.words(\"english\"))\n",
    "        text = [w for w in text if not w in stops]\n",
    "    \n",
    "    text = \" \".join(text)\n",
    "\n",
    "    # Clean the text\n",
    "    \n",
    "    text = re.sub(r\"\\bappeared in the.*\",'',text)\n",
    "    \n",
    "    text = re.sub(r\"what ' s\", \"what is \", text)\n",
    "    # my turn\n",
    "    text = re.sub(r\"he's\", \"he is\", text)\n",
    "    text = re.sub(r\"she's\", \"she is\", text)\n",
    "    text = re.sub(r\"it's\", \"it is\", text)\n",
    "    text = re.sub(r\"let's\", \"let us\", text)\n",
    "    text = re.sub(r\"can’t\", \"can not \", text)\n",
    "    text = re.sub(r\"you’ll\", \"you will\", text)\n",
    "    text = re.sub(r\"he’ll\", \"he will\", text)\n",
    "    text = re.sub(r\"she’ll\", \"she will\", text)\n",
    "    \n",
    "    text = re.sub(r\"didn't\", \"did not\", text)\n",
    "    text = re.sub(r\"don't\", \"do not\", text)\n",
    "    text = re.sub(r\"won't\", \"will not\", text)\n",
    "    text = re.sub(r\"wouldn't\", \"would not\", text)\n",
    "    text = re.sub(r\"shouldn't\", \"should not\", text)\n",
    "    text = re.sub(r\"doesn't\", \"does not\", text)\n",
    "    text = re.sub(r\"aren’t\", \"are not\", text)\n",
    "    text = re.sub(r\"isn't\", \"is not\", text)\n",
    "    text = re.sub(r\"doesn't\", \"does not\", text)\n",
    "    text = re.sub(r\"could't\", \"could not\", text)\n",
    "    text = re.sub(r\"weren't\", \"were not\", text)\n",
    "    text = re.sub(r\"hasn't\", \"has not\", text)\n",
    "\n",
    "    \n",
    "    text = re.sub(r\"u\\.s\\.\",'american',text)\n",
    "    #text = re.sub(r\"[^A-Za-z0-9,!+^.\\/'-=?.-]\", \"  \", text) # origin is [^A-Za-z0-9^,!.\\/'+-=?]\n",
    "    \n",
    "    text = re.sub(r\"4get\", \"forget \", text)\n",
    "    text = re.sub(r\"coo[o]+\", \"cooo\", text)\n",
    "    text = re.sub(r\"so[o]+\", \"sooo \", text)\n",
    "    text = re.sub(r\" [0-9]+ : [0-9]+\", \" aa:bb \", text) #remove time\n",
    "    text = re.sub(r\" [0-9]+\", \" \", text) #remove number\n",
    "    text = re.sub(r\"\\?\", \" ? \", text)\n",
    "    text = re.sub(r\"\\.\\.[\\.]+\", \" a... \", text)\n",
    "    text = re.sub(r\"uh[h]+\", \" uh \", text)\n",
    "    text = re.sub(r\"zz[z]+\", \" zzz \", text)\n",
    "    text = re.sub(r\"loo[o]+l\", \" lool \", text)\n",
    "    text = re.sub(r\" \\.\\.\", \" \", text)\n",
    "    \n",
    "    #end my turn\n",
    "    text = re.sub(r\"\\' s\", \" \", text)\n",
    "    text = re.sub(r\"\\' ve\", \" have \", text)\n",
    "    text = re.sub(r\"can ' t\", \"can not \", text)\n",
    "    text = re.sub(r\"n ' t\", \" not \", text)\n",
    "    text = re.sub(r\"i ' m\", \"i am \", text)\n",
    "    text = re.sub(r\"\\' re\", \" are \", text)\n",
    "    text = re.sub(r\"\\' d\", \" would \", text)\n",
    "    text = re.sub(r\"\\' ll\", \" will \", text)\n",
    "    text = re.sub(r\",\", \" \", text)\n",
    "    text = re.sub(r\" \\.\", \" \", text)\n",
    "    \n",
    "    text = re.sub(r\"!![!]+\", \" !!! \", text)\n",
    "    text = re.sub(r\"\\/\", \" \", text)\n",
    "    text = re.sub(r\"\\^\", \" ^ \", text)\n",
    "    text = re.sub(r\"\\+\", \" + \", text)\n",
    "    text = re.sub(r\"e - mail\", \"email\", text)\n",
    "    text = re.sub(r\"\\-\", \" - \", text)\n",
    "    text = re.sub(r\" \\=\", \" \", text)\n",
    "    text = re.sub(r\"'\", \" \", text)\n",
    "    text = re.sub(r\"(\\d+)(k)\", r\"\\g<1>000\", text)\n",
    "    text = re.sub(r\" : \", \" : \", text)\n",
    "    text = re.sub(r\" e g \", \" eg \", text)\n",
    "    text = re.sub(r\" b g \", \" bg \", text)\n",
    "    text = re.sub(r\" u s \", \" american \", text)\n",
    "    #text = re.sub(r\"\\0s\", \"0\", text)\n",
    "    #text = re.sub(r\" 9 11 \", \"911\", text)\n",
    "    text = re.sub(r\"j k\", \"jk\", text)\n",
    "    text = re.sub(r\"\\s{2,}\", \" \", text)\n",
    "    \n",
    "    text = re.sub('”?appeared in the [a-z]* [0-9]*, [0-9]*, print edition as.*',' ',text)\n",
    "    \n",
    "    # remove weird punctuation\n",
    "    \n",
    "    # prevent two words binded \n",
    "    #for i in punctuation:\n",
    "        #text = text.replace(i,\" \")\n",
    "    #text = text.replace('  ','')\n",
    "   \n",
    "    \n",
    "    # Optionally, shorten words to their stems\n",
    "    if stem_words is True:\n",
    "        text = text.split()\n",
    "        stemmer = SnowballStemmer('english')\n",
    "        stemmed_words = [stemmer.stem(word) for word in text]\n",
    "        text = \" \".join(stemmed_words)\n",
    "    \n",
    "    token_list = []\n",
    "    for i in word_tokenize(text):\n",
    "        i = re.sub(r\"\\d+\", \"\", i)\n",
    "        i = re.sub(r'\\.',' ',i)\n",
    "        if len(i) > 1:\n",
    "            i = re.sub(r\"[^A-Za-z0-9,!+^.'\\/,'-=?-]\", \"  \", i)\n",
    "            i = i.strip()\n",
    "            i = i.replace(' ','')\n",
    "            if len(i) > 1:\n",
    "                #remove number\n",
    "                token_list.append(i)\n",
    "    text = ' '.join(token for token in token_list)\n",
    "    \n",
    "    \n",
    "    text = re.sub(r\"do n't\",'do not',text)\n",
    "    text = re.sub(r\"did n't\",'do not',text)\n",
    "\n",
    "    # Return a list of words\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['2016', '2017']"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_path = '/Users/mueric35/Downloads/wsj_haha/data'\n",
    "selected_year = [str(i) for i in range(2016,2018)]\n",
    "selected_year"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# map function\n",
    "def tidy_text(key,texts_dict):\n",
    "    result = text_to_wordlist(texts_dict.get(key)['paragraph'], stem_words = False, remove_stopwords= True)\n",
    "    return(result)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_year(selected_year,data_path):\n",
    "    \n",
    "    path = data_path\n",
    "    for year in os.listdir(path):\n",
    "        \n",
    "        if year in selected_year:\n",
    "            print('Working on ' + str(year))\n",
    "            \n",
    "            pk_year_path = path + '/' + year + '/' + 'wsj_' + year + '_dic.pkl'\n",
    "            print('Loading: ' + pk_year_path)\n",
    "            \n",
    "            texts =  open(pk_year_path,'rb') \n",
    "            texts_dict = pickle.load(texts)\n",
    "            keys = list(texts_dict.keys())\n",
    "\n",
    "            # save file to input new data and key for map funciton\n",
    "            tidy_texts_dict = texts_dict\n",
    "            \n",
    "            results = []\n",
    "            keys = texts_dict.keys()\n",
    "            \n",
    "            start_time = time.time()\n",
    "            for key in texts_dict.keys():\n",
    "                tidy_texts_dict.get(key)['paragraph'] = tidy_text(key,texts_dict)\n",
    "            \n",
    "            print(\"--- %s seconds ---\" % (time.time() - start_time))\n",
    "            \n",
    "          \n",
    "            print('Finished tidying')\n",
    "            save_path =  path + '/' + year + '/' + 'tidy_wsj_' + year + '_dic.pkl'\n",
    "            print('Saving to ' + save_path + \"\\n\")\n",
    "            with open(save_path, 'wb') as handle:\n",
    "                pickle.dump(tidy_texts_dict, handle, protocol=pickle.HIGHEST_PROTOCOL)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Working on 2016\n",
      "Loading: /Users/mueric35/Downloads/wsj_haha/data/2016/wsj_2016_dic.pkl\n",
      "--- 354.1629378795624 seconds ---\n",
      "Finished tidying\n",
      "Saving to /Users/mueric35/Downloads/wsj_haha/data/2016/tidy_wsj_2016_dic.pkl\n",
      "\n",
      "Working on 2017\n",
      "Loading: /Users/mueric35/Downloads/wsj_haha/data/2017/wsj_2017_dic.pkl\n",
      "--- 293.4192051887512 seconds ---\n",
      "Finished tidying\n",
      "Saving to /Users/mueric35/Downloads/wsj_haha/data/2017/tidy_wsj_2017_dic.pkl\n",
      "\n"
     ]
    }
   ],
   "source": [
    "clean_year(selected_year,data_path)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
